{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7436d18e-0191-45c2-b7f4-22df7a27e89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jayaprakash/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jayaprakash/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jayaprakash/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/jayaprakash/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Libraries imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd  # For loading and working with CSV data (our resume dataset)\n",
    "import nltk  # Natural Language Toolkit - main NLP library\n",
    "from nltk.corpus import stopwords  # To access list of common stop words (the, is, a, etc.)\n",
    "from nltk.tokenize import word_tokenize  # Function to split text into words (tokenization)\n",
    "import string  # To access punctuation marks (!@#$%^&*().,;:'\")\n",
    "import re  # Regular expressions - for pattern matching and text cleaning\n",
    "\n",
    "# Download NLTK data packages (run once to download to your computer)\n",
    "nltk.download('punkt')  # Downloads tokenization rules (how to split sentences/words)\n",
    "nltk.download('stopwords')  # Downloads list of stop words in multiple languages\n",
    "nltk.download('wordnet')  # Downloads word meanings database (for lemmatization later)\n",
    "# Download the missing punkt_tab data\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")  # Confirmation message that all imports worked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6916d400-12ad-48ad-b934-4ae742cb8685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Python Developer with 5+ years experience! Email: test@gmail.com\n",
      "Cleaned: python developer with years experience email testgmailcom\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION 1: Clean text (lowercase + remove special characters + remove extra spaces)\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans resume text by:\n",
    "    1. Converting to lowercase\n",
    "    2. Removing special characters and numbers\n",
    "    3. Removing extra spaces\n",
    "    \"\"\"\n",
    "    # Convert entire text to lowercase (Python → python)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and numbers using regex\n",
    "    # [^a-z\\s] means \"keep only letters (a-z) and spaces (\\s)\"\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    \n",
    "    # Remove extra spaces (multiple spaces become single space)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Remove leading/trailing spaces\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text  # Return the cleaned text\n",
    "\n",
    "# Test the function with a sample\n",
    "sample_text = \"Python Developer with 5+ years experience! Email: test@gmail.com\"\n",
    "cleaned = clean_text(sample_text)\n",
    "print(\"Original:\", sample_text)\n",
    "print(\"Cleaned:\", cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2da9df76-3fd7-4a40-9cb2-32ac6b125b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: python developer with years experience email testgmailcom\n",
      "After removing stop words: python developer years experience email testgmailcom\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION 2: Remove stop words (common words like the, is, a, with, etc.)\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"\n",
    "    Removes stop words from text:\n",
    "    1. Tokenizes (splits into words)\n",
    "    2. Filters out stop words\n",
    "    3. Joins back into sentence\n",
    "    \"\"\"\n",
    "    # Get English stop words from NLTK (the, is, a, an, with, etc.)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Split text into individual words (tokenization)\n",
    "    words = text.split()\n",
    "    \n",
    "    # Keep only words that are NOT in stop words list\n",
    "    # List comprehension: [word for word in words if condition]\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Join words back into a sentence with spaces\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Test the function\n",
    "sample_text = \"python developer with years experience email testgmailcom\"\n",
    "filtered = remove_stop_words(sample_text)\n",
    "print(\"Before:\", sample_text)\n",
    "print(\"After removing stop words:\", filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8411558-3e3c-498c-810c-98084d6b5d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: python developer years experience email testgmailcom\n",
      "Tokenized (list of words): ['python', 'developer', 'years', 'experience', 'email', 'testgmailcom']\n",
      "Number of tokens: 6\n"
     ]
    }
   ],
   "source": [
    "# FUNCTION 3: Tokenize text (split into list of words)\n",
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Splits text into individual words (tokens)\n",
    "    Returns a list of words\n",
    "    \"\"\"\n",
    "    # Use NLTK's word_tokenize to split text into words\n",
    "    # More sophisticated than just .split() - handles punctuation better\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    return tokens  # Returns list of words\n",
    "\n",
    "# Test the function\n",
    "sample_text = \"python developer years experience email testgmailcom\"\n",
    "tokens = tokenize_text(sample_text)\n",
    "print(\"Original text:\", sample_text)\n",
    "print(\"Tokenized (list of words):\", tokens)\n",
    "print(f\"Number of tokens: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9fd4021-38ba-4204-aa0a-e0813c1621f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ORIGINAL RESUME (first 300 chars) ===\n",
      "         HR ADMINISTRATOR/MARKETING ASSOCIATE\n",
      "\n",
      "HR ADMINISTRATOR       Summary     Dedicated Customer Service Manager with 15+ years of experience in Hospitality and Customer Service Management.   Respected builder and leader of customer-focused teams; strives to instill a shared, enthusiastic commit\n",
      "\n",
      "==================================================\n",
      "\n",
      "STEP 1: Clean text\n",
      "hr administratormarketing associate hr administrator summary dedicated customer service manager with years of experience in hospitality and customer service management respected builder and leader of \n",
      "\n",
      "==================================================\n",
      "\n",
      "STEP 2: Remove stop words\n",
      "hr administratormarketing associate hr administrator summary dedicated customer service manager years experience hospitality customer service management respected builder leader customerfocused teams \n",
      "\n",
      "==================================================\n",
      "\n",
      "STEP 3: Tokenize\n",
      "First 20 tokens: ['hr', 'administratormarketing', 'associate', 'hr', 'administrator', 'summary', 'dedicated', 'customer', 'service', 'manager', 'years', 'experience', 'hospitality', 'customer', 'service', 'management', 'respected', 'builder', 'leader', 'customerfocused']\n",
      "Total tokens: 494\n"
     ]
    }
   ],
   "source": [
    "# Load the resume dataset that we explored in Day 4\n",
    "df = pd.read_csv('Resume.csv')  # Read CSV file from current folder into pandas dataframe\n",
    "\n",
    "# Get one real resume from the dataset to test our functions\n",
    "sample_resume = df['Resume_str'][0]  # Get first resume (index 0) from Resume_str column\n",
    "\n",
    "# Show original resume (first 300 characters only so it's not too long)\n",
    "print(\"=== ORIGINAL RESUME (first 300 chars) ===\")\n",
    "print(sample_resume[:300])  # [:300] means show characters 0 to 299\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")  # Print blank line and separator line\n",
    "\n",
    "# STEP 1: Apply clean_text function (lowercase + remove special chars)\n",
    "print(\"STEP 1: Clean text\")\n",
    "cleaned = clean_text(sample_resume)  # Call our function from earlier\n",
    "print(cleaned[:200])  # Show first 200 characters of cleaned text\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator\n",
    "\n",
    "# STEP 2: Apply remove_stop_words function (remove the, is, a, etc.)\n",
    "print(\"STEP 2: Remove stop words\")\n",
    "no_stop_words = remove_stop_words(cleaned)  # Remove common meaningless words\n",
    "print(no_stop_words[:200])  # Show first 200 characters\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")  # Separator\n",
    "\n",
    "# STEP 3: Apply tokenize_text function (split into list of words)\n",
    "print(\"STEP 3: Tokenize\")\n",
    "tokens = tokenize_text(no_stop_words)  # Convert sentence to list of words\n",
    "print(\"First 20 tokens:\", tokens[:20])  # Show first 20 words from the list\n",
    "print(f\"Total tokens: {len(tokens)}\")  # Count how many words total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0818799-921f-4cc8-8162-5328fb048238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original resume length: 5480 characters\n",
      "Processed tokens: 488 words\n",
      "\n",
      "First 30 tokens: ['hr', 'generalist', 'summary', 'dedicated', 'focused', 'administrative', 'assistant', 'excels', 'prioritizing', 'completing', 'multiple', 'tasks', 'simultaneously', 'following', 'achieve', 'project', 'goals', 'seeking', 'role', 'increased', 'responsibility', 'authority', 'highlights', 'microsoft', 'office', 'proficiency', 'excel', 'spreadsheets', 'meticulous', 'attention']\n"
     ]
    }
   ],
   "source": [
    "# COMBINED FUNCTION: Does all preprocessing steps at once\n",
    "def preprocess_resume(text):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for resume text:\n",
    "    1. Clean text (lowercase + remove special chars)\n",
    "    2. Remove stop words (common words like the, is, a)\n",
    "    3. Tokenize (split into list of words)\n",
    "    \n",
    "    Input: Raw resume text (string)\n",
    "    Output: List of cleaned words (list)\n",
    "    \"\"\"\n",
    "    # Step 1: Clean the text\n",
    "    text = clean_text(text)\n",
    "    \n",
    "    # Step 2: Remove stop words\n",
    "    text = remove_stop_words(text)\n",
    "    \n",
    "    # Step 3: Tokenize into list of words\n",
    "    tokens = tokenize_text(text)\n",
    "    \n",
    "    return tokens  # Return final list of cleaned words\n",
    "\n",
    "# Test the combined function on a sample resume\n",
    "sample = df['Resume_str'][5]  # Get resume at index 5\n",
    "processed = preprocess_resume(sample)  # Apply all steps at once\n",
    "\n",
    "print(\"Original resume length:\", len(sample), \"characters\")\n",
    "print(\"Processed tokens:\", len(processed), \"words\")\n",
    "print(\"\\nFirst 30 tokens:\", processed[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79970452-2a9e-4a61-9fd8-dbca76187ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ preprocessing.py file created successfully!\n",
      "✅ All functions saved!\n"
     ]
    }
   ],
   "source": [
    "# Save all preprocessing functions to a Python file\n",
    "# This creates preprocessing.py in your project folder\n",
    "\n",
    "code = '''\n",
    "# preprocessing.py\n",
    "# Text preprocessing functions for Resume Analyzer project\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "# FUNCTION 1: Clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"Converts to lowercase and removes special characters\"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^a-z\\\\s]', '', text)  # Keep only letters and spaces\n",
    "    text = re.sub(r'\\\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = text.strip()  # Remove leading/trailing spaces\n",
    "    return text\n",
    "\n",
    "# FUNCTION 2: Remove stop words\n",
    "def remove_stop_words(text):\n",
    "    \"\"\"Removes common words like the, is, a, with\"\"\"\n",
    "    stop_words = set(stopwords.words('english'))  # Get English stop words\n",
    "    words = text.split()  # Split into words\n",
    "    filtered_words = [word for word in words if word not in stop_words]  # Filter\n",
    "    return ' '.join(filtered_words)  # Join back\n",
    "\n",
    "# FUNCTION 3: Tokenize text\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Splits text into list of words\"\"\"\n",
    "    tokens = word_tokenize(text)  # Tokenize using NLTK\n",
    "    return tokens\n",
    "\n",
    "# COMBINED FUNCTION: Complete preprocessing\n",
    "def preprocess_resume(text):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline:\n",
    "    1. Clean text\n",
    "    2. Remove stop words\n",
    "    3. Tokenize\n",
    "    Returns: List of cleaned words\n",
    "    \"\"\"\n",
    "    text = clean_text(text)  # Step 1\n",
    "    text = remove_stop_words(text)  # Step 2\n",
    "    tokens = tokenize_text(text)  # Step 3\n",
    "    return tokens\n",
    "'''\n",
    "\n",
    "# Write to file\n",
    "with open('preprocessing.py', 'w') as f:\n",
    "    f.write(code)\n",
    "\n",
    "print(\"✅ preprocessing.py file created successfully!\")\n",
    "print(\"✅ All functions saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcea5752-714a-4239-a0bd-1d9bfea1bb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f26fa30-a118-48a7-bc20-5bb5a0058d39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
